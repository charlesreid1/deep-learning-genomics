{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojiEwxAcx0nb"
   },
   "source": [
    "# Keras 1D Convolutional Neural Net for DNA Transcription Prediction\n",
    "\n",
    "In this notebook we walk through the use of a 1D convolutional neural network to make predictions about DNA sequences. We utilize an example from Chapter 6 of the book Deep Learning for Genomics by Ramsundar et al (OReilly).\n",
    "\n",
    "The example covered in Chapter 6 is the problem of predicting where in a sequence of DNA the RNA transcriptase will bind to DNA and begin the transcription process. The data set consists of DNA (a 1-dimensional sequence of characters from a 4-letter alphabet) and a binary label - is this sequence a transcription factor binding site or not. \n",
    "\n",
    "From a deep learning perspective, the problem is interesting for a number of reasons:\n",
    "\n",
    "* Convolutional neural networks are commonly-used deep learning architecture\n",
    "* Dealing with 1D data is useful\n",
    "* Requires turning DNA sequences into data that can be fed to a deep learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EX8dMlAsnXj"
   },
   "source": [
    "## Imports\n",
    "\n",
    "Here are the import statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Okf1AJxXbvW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For loading data sets\n",
    "import joblib\n",
    "\n",
    "# Useful sklearn metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXaxRAyaEgb0"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TgZec1_7uWK6",
    "outputId": "f67cb4af-ae8f-4ab8-e053-359d7befff63"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulv1xNd9IEa8"
   },
   "outputs": [],
   "source": [
    "# Make sure we're using the GPU\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_ZmE1pq0Rdf"
   },
   "source": [
    "# Transcription Factor Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BtASXPfod9BK"
   },
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This example will use a data set about a [transcription factor called JunD](https://www.uniprot.org/uniprot/P17535). \n",
    "\n",
    "The data set used in this example comes from the human chromosome 22 (the smallest human chromosome). It is 50 Mbp.\n",
    "\n",
    "The chromosome is broken into short segments of 101 base pairs.\n",
    "\n",
    "One hot encoding is used to represent the sequences (i.e., each base pair is turned into a vector of 0s and 1s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "jXfwn_SgkVVs",
    "outputId": "02797766-8605-4179-a4d5-435f76c21108"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RVPiF3TkYw7"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhKIRcW2kSp-"
   },
   "source": [
    "## 1D Convolutional Neural Networks\n",
    "\n",
    "Beacuse we are dealing with a 1D sequence of data, we use a convolutional neural network (CNN) that performs the convolution in 1 dimension (1D CNN).\n",
    "\n",
    "(A 2D convolutional neural network would be good for spatial or image data, while a 3D convolutional neural network would be good for volumetric data.)\n",
    "\n",
    "Typically, 1D convolutional neural networks have the following layers:\n",
    "\n",
    "* Convolution layer, or (for 2D CNN) multiple convolution layers followed by a pooling layer\n",
    "\n",
    "* Dropout layer (optional)\n",
    "\n",
    "* Flatten layer (all output becomes 1D)\n",
    "\n",
    "* Dense layer (everything-connected-to-everything network; number of dense output neurons = number of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTlyUojzhDh0"
   },
   "source": [
    "**Strategy:** We start by implementing a 1D CNN using the architecture in Chapter 6 of <u>Deep Learning for the Life Sciences</u>. We then implement another 1D CNN model using an example from the [Keras documentation](https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions) for sequence classification using Keras. It uses a different sequence of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YPmWItnDg6Fq"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Before we get to the CNN layers, we start by loading the data, bypassing the `DiskDataset` provided by DeepChem and loading the data set directly into memory with `joblib`.\n",
    "\n",
    "**NOTE:** If the data set were actually large enough to require it to be on disk we would need to use a generator function. We illustrate how to do that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTlq6lKdIfjj"
   },
   "outputs": [],
   "source": [
    "def load_dataset(which_dataset):\n",
    "    import os\n",
    "    \n",
    "    if which_dataset not in ['train','valid','test']:\n",
    "        raise Exception(\"Error: can only load datasets 'train', 'valid', or 'test', you specified %s\"%(which_dataset))\n",
    "    \n",
    "    # Set data directory name\n",
    "    data_dir = \"%s_dataset\"%(which_dataset)\n",
    "    \n",
    "    # Set filenames for X, y, w, labels\n",
    "    base_filename = \"shard-0-%s.joblib\"\n",
    "    X_filename    = os.path.join(data_dir,base_filename%(\"X\"))\n",
    "    y_filename    = os.path.join(data_dir,base_filename%(\"y\"))\n",
    "    w_filename    = os.path.join(data_dir,base_filename%(\"w\"))\n",
    "    ids_filename  = os.path.join(data_dir,base_filename%(\"ids\"))\n",
    "    \n",
    "    X = joblib.load(X_filename)\n",
    "    y = joblib.load(y_filename)\n",
    "    w = joblib.load(w_filename)\n",
    "    ids = joblib.load(ids_filename)\n",
    "    \n",
    "    return X, y, w, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "irfGJhQZQpbM",
    "outputId": "8de13a6d-5e91-403f-b323-9133f1571803"
   },
   "outputs": [],
   "source": [
    "train = load_dataset('train')\n",
    "valid = load_dataset('valid')\n",
    "test = load_dataset('test')\n",
    "\n",
    "X_train, y_train, w_train, ids_train = load_dataset('train')\n",
    "X_valid, y_valid, w_valid, ids_valid = load_dataset('valid')\n",
    "X_test, y_test, w_test, ids_test = load_dataset('test')\n",
    "\n",
    "n_train_obs = np.shape(X_train)[0]\n",
    "n_valid_obs = np.shape(X_valid)[0]\n",
    "n_test_obs = np.shape(X_test)[0]\n",
    "\n",
    "w_train = np.squeeze(w_train)\n",
    "w_valid = np.squeeze(w_valid)\n",
    "w_test = np.squeeze(w_test)\n",
    "\n",
    "print('-'*20)\n",
    "print('training data shape:')\n",
    "print(np.shape(X_train))\n",
    "print(np.shape(y_train))\n",
    "print('-'*20)\n",
    "print('validation data shape:')\n",
    "print(np.shape(X_valid))\n",
    "print(np.shape(y_valid))\n",
    "print('-'*20)\n",
    "print('test data shape:')\n",
    "print(np.shape(X_test))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "otEgkLNBu7Ov"
   },
   "source": [
    "The $y$ vector contains the binary output 0 or 1.\n",
    "\n",
    "The authors of the book have provided the data - the 0s and 1s of the y vector - as 64-bit integers. They probably could have used 8-bit integers and saved some space. Someone should submit a pull request to [deepchem/DeepLearningLifeSciences](https://github.com/deepchem/DeepLearningLifeSciences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "367znoXkLcfa",
    "outputId": "58ef12be-870a-4088-d23c-abb3b3345795"
   },
   "outputs": [],
   "source": [
    "print(type(y_test[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L74zNl8Du9Oi",
    "outputId": "9d3bb29c-6940-4275-f387-fd144fdd39a5"
   },
   "outputs": [],
   "source": [
    "print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zljueQJ3xfq9"
   },
   "source": [
    "Also important to note - the data set is very skewed toward one class (the negative class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "43MECmQtxqYz",
    "outputId": "484de429-fe05-4a52-df29-a35902555649"
   },
   "outputs": [],
   "source": [
    "print(np.sum(y_train==0))\n",
    "print(np.sum(y_train==1))\n",
    "print(np.sum(y_train==1)/np.sum(y_train==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "M73H_HfZxkHE",
    "outputId": "b1a84030-f31e-41f9-aa65-fefe9b674625"
   },
   "outputs": [],
   "source": [
    "print(np.sum(y_test==0))\n",
    "print(np.sum(y_test==1))\n",
    "print(np.sum(y_test==1)/np.sum(y_test==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfaJV4Oi-rqy"
   },
   "source": [
    "### CNN Architecture\n",
    "\n",
    "The layers of the DeepChem Chapter 6 example network are as follows:\n",
    "\n",
    "* Conv1D layer\n",
    "* Dropout layer\n",
    "* Conv1D layer\n",
    "* Dropout layer\n",
    "* Conv1D layer\n",
    "* Dropout layer\n",
    "* Flatten layer\n",
    "* Dense layer (1 perceptron)\n",
    "\n",
    "As before, the last layer has a single perceptron that outputs the 0 or 1 binary classification prediction.\n",
    "\n",
    "(Also note, number of filters is typically a power of 2, so rather than using the book's 15 channels we have used 16 channels.)\n",
    "\n",
    "A few Keras documentation links helpful for translating from Keras to DeepChem:\n",
    "* [Conv1D layer](https://keras.io/layers/convolutional/)\n",
    "* [MaxPooling1D layer](https://keras.io/layers/pooling/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JRF3ostwxCYI"
   },
   "source": [
    "Side note: before defining the convolutional neural net, we should define a few metrics we are interested in first. We utilize two metrics, the accuracy and the F-value, defined as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{Accuracy} = \\dfrac{\\mbox{TP} + \\mbox{TN}}{\\mbox{TP} + \\mbox{FP} + \\mbox{TN} + \\mbox{FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{Precision} = \\dfrac{ \\mbox{TP} }{ \\mbox{TP} + \\mbox{FP} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{Recall} = \\dfrac{ \\mbox{TP} }{ \\mbox{TP} + \\mbox{FN} }\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mbox{F-value} = \\dfrac{ 2 \\mbox{Recall} \\times \\mbox{Precision} }{ \\mbox{Recall} + \\mbox{Precision} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1OF4uOI8yQCp"
   },
   "source": [
    "**Accuracy:** We expect the accuracy to be extremely high - accuracy takes into account both correct positive guesses and correct negative guesses, and any classifier could achieve high accuracy by simply guessing 0 all the time.\n",
    "\n",
    "**Precision:** We are more interested in the precision than accuracy, as it defines a metric that focuses only on the positive classification task. We expect this to be a better measure of the model's accuracy. (Also note the ROC curve plots TP vs FP so the ROC curve can be thought of as a graphical representation of the precision.) This is a measure of how well the model did _when it guessed positive_.\n",
    "\n",
    "**Recall:** Recall accounts for false negatives as well as true positives - that is, examples that should have been labeled positive but were missed by the model. This is a measure of how well the model did _on all the positive examples_.\n",
    "\n",
    "**F-value:** This measure is the _harmonic mean_ of precision and recall. Because recall increases as the model guesses less conservatively, and precision increases as it guesses more conservatively, there must be a tradeoff. The F-value is the harmonic mean of the two. Recall and precision will always be bounded from 0 to 1. Here is a contour plot of the harmonic mean function $f(x,y) = \\dfrac{2xy}{x+y}$ on the unit square:\n",
    "\n",
    "![contour of z = (2xy)/(x+y)](https://raw.githubusercontent.com/charlesreid1/dib-deep-chem/master/img/contour.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkCybmPn000I"
   },
   "source": [
    "A higher F score indicates an optimum tradeoff between recall (not making the positive call when you should) and precision (being correct when you make the positive call).\n",
    "\n",
    "Custom Keras metrics can be defined and will be computed at the end of each epoch. A metric is just a function that takes two arguments - the true $y$ values, and the predicted $y$ values. Accuracy is already built in to Keras. We define precision, recall, and F-value below.\n",
    "\n",
    "(Note that we define these as functions and use them as metrics, but they could also be implemented as [Callback objects](https://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2) that have an initialization method `on_train_begin()` and a method run at the end of each epoch, `on_epoch_end()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hcbf1MxL92TQ"
   },
   "source": [
    "## Definitions of Custom Keras Metrics\n",
    "\n",
    "To compute metrics at each epoch (i.e., at the end of each training iteration), we define a function that takes a vector of true values and a vector of predicted values, and use those to compute and return the mtetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8y-SZMjp1Lzu"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Xn0IJwO2_7O"
   },
   "source": [
    "Now we can pass those function handles to the `model.compile()` call (see last line below).\n",
    "\n",
    "Note that `sample_weight_mode=None` will utilize sample-wise weights, which is what is provided in the data set (see the [keras documentation on models](https://keras.io/models/model/) for details)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "EpKSj2OD6MdR",
    "outputId": "f2e24709-2580-4382-e50d-d88a849839a2"
   },
   "outputs": [],
   "source": [
    "features = 4\n",
    "seq_length = 101\n",
    "convolution_window = 10\n",
    "n_filters = 16\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Unroll the book's for loop\n",
    "\n",
    "# Convolution layer 1\n",
    "model.add(Conv1D(n_filters, convolution_window, \n",
    "                 activation='relu', padding='same', \n",
    "                 input_shape=(seq_length, features)))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Convolution layer 2\n",
    "model.add(Conv1D(n_filters, convolution_window, \n",
    "                 activation='relu', padding='same'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Convolution layer 3\n",
    "model.add(Conv1D(n_filters, convolution_window, \n",
    "                 activation='relu', padding='same'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Flatten to 1D\n",
    "model.add(Flatten())\n",
    "\n",
    "# Shrink to 1 neuron for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              sample_weight_mode=None,\n",
    "              metrics=['accuracy',\n",
    "                       precision,\n",
    "                       recall,\n",
    "                       fvalue])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DpIyvk3uxhg"
   },
   "source": [
    "**Side Note:** as per [this Keras issue](https://github.com/keras-team/keras/issues/3230), the ROC AUC cannot be implemented directly as a metric by Keras because (1) it is a global metric and can therefore be misleading to use as a metric to monitor during iterations, and (2) for keras to use it as a loss function (use it as the objective function) it must be differentiable.\n",
    "\n",
    "We show how to plot the ROC curve in post-processing below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPsF-FKuN2XC"
   },
   "source": [
    "(Note: [useful issue/thread on sample weights](https://github.com/keras-team/keras/issues/3653) in the keras repo. It was important to _both_ use the \"adam\" optimizer _and_ set the sample weights as below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swfa1k_npE_-"
   },
   "source": [
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1843
    },
    "colab_type": "code",
    "id": "iBRXjZujpH5z",
    "outputId": "08675307-96d9-4cac-f6b6-75a6e42bc6e9"
   },
   "outputs": [],
   "source": [
    "weights_file_A = os.path.join('models','keras_cnn1d_dna_transcription_A.h5')\n",
    "if os.path.exists(weights_file_A):\n",
    "    model.load_weights(weights_file_A)\n",
    "else:\n",
    "    # Train the model, then evaluate/save the ROC curve\n",
    "    num_epochs = 50\n",
    "    hist = model.fit(X_train, y_train,\n",
    "                     sample_weight = w_train,\n",
    "                     batch_size=2048,\n",
    "                     epochs=num_epochs,\n",
    "                     verbose = 1,\n",
    "                     validation_data=(X_valid, y_valid, w_valid))\n",
    "    model.save_weights(weights_file_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxcLayL14kn_"
   },
   "source": [
    "Visualization of the loss function and metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "B5Yl6hjs4ouX",
    "outputId": "ff646719-a060-4d69-eb9a-abaf0219af55"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('Loss Rate (Ch 6 Model, Seq Data Only)', size=14)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Training interations')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title('Accuracy Rate (Ch 6 Model, Seq Data Only)', size=14)\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "1mTa8Ds56LeU",
    "outputId": "93656cd1-f950-4ddb-dbd7-141b49c0b0b9"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(hist.history['precision'])\n",
    "    plt.plot(hist.history['val_precision'])\n",
    "    plt.title('Precision (Ch 6 Model, Seq Data Only)', size=14)\n",
    "    plt.ylabel('Precision %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(hist.history['recall'])\n",
    "    plt.plot(hist.history['val_recall'])\n",
    "    plt.title('Recall (Ch 6 Model, Seq Data Only)', size=14)\n",
    "    plt.ylabel('Recall %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "q9CmCYXH6rbM",
    "outputId": "2d7b7ac8-2a65-4a9a-9877-a733235ef5dc"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(hist.history['fvalue'])\n",
    "    plt.plot(hist.history['val_fvalue'])\n",
    "    plt.title('F-Value', size=14)\n",
    "    plt.ylabel('F-Value (Ch 6 Model, Seq Data Only)')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8Prp_9wpWEe"
   },
   "source": [
    "Now we can visualize the performance of the binary classification task using the ROC (receiver operating characteristic) curve, which shows true positives versus false positives - focusing on the O(100) positive instances, rather than all 30,000+ data points. The closer to x = y the curve gets, the worse it performs. Ideally we have a large area under the curve.\n",
    "\n",
    "The sklearn package has the ROC curve as a built-in metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "RnpN_Js0OxXU",
    "outputId": "46d3943c-fd80-494c-c571-058bae07c573"
   },
   "outputs": [],
   "source": [
    "# Useful function for plotting the ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get model predictions on test data\n",
    "y_model = model.predict(X_test).ravel()\n",
    "\n",
    "# Form the ROC curve values\n",
    "fpr, tpr, _ = roc_curve(y_test, y_model)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve (Ch 6 Model, Seq Data Only)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GcN3JmJMrE_p"
   },
   "source": [
    "If we call the `evaluate()` method on the model, it returns the loss value and metrics values for the model in test mode.\n",
    "\n",
    "Here are the loss value and accuracy for our model run on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "j_Iqils1RFeF",
    "outputId": "e008ad13-0cb1-417b-a084-e6060c62c208"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Loss value: %0.4f\"%(scores[0]))\n",
    "print(\"Accuracy: %0.4f\"%(scores[1]))\n",
    "print(\"Precision: %0.4f\"%(scores[2]))\n",
    "print(\"Recall: %0.4f\"%(scores[3]))\n",
    "print(\"F-Value: %0.4f\"%(scores[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTAs6xuNa3eA"
   },
   "source": [
    "We can also plot the confusion matrix to get a sense of how well our model is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NwW9f7jsavl7",
    "outputId": "c34818ce-333d-4c0b-858b-53ef0e5d3bf3"
   },
   "outputs": [],
   "source": [
    "# True values: y_test\n",
    "# Pred values: y_model\n",
    "cm = confusion_matrix(y_test, np.round(y_model))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ax, y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    A function to plot the confusion matrix.\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        title = \"Confusion Matrix\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), \n",
    "             rotation=45,\n",
    "             ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else ','\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", \n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "yqzqu-Kxc3jh",
    "outputId": "84ccc0b5-af35-4fad-a639-43ada952db06"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "plot_confusion_matrix(ax, y_test, np.round(y_model), classes=['N','Y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsjpoNjVdWh9"
   },
   "source": [
    "We can also look at the distribution of outputs from the model, which are between 0 and 1 (we round them to get the guessed class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "2UnYTEj5ASMG",
    "outputId": "e985e717-e283-4022-ac41-4240e4cea0ff"
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6HtqjirWU0L"
   },
   "source": [
    "# Using the Keras Example 1D CNN\n",
    "\n",
    "The convolutional neural network proposed in Chapter 6 does not match the 1D convolutional neural net example contained in the Keras documentation [here](https://keras.io/getting-started/sequential-model-guide/#sequence-classification-with-1d-convolutions), so before we move on to adding chromatin data, we try the Keras architecture, and see if there is a significant difference in performance.\n",
    "\n",
    "**NOTE:** The main difference between the Keras 1D CNN and the Chapter 6 1D CNN is, the Chapter 6 1D CNN uses more dropout layers to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ln0TxKssWToX"
   },
   "outputs": [],
   "source": [
    "features = 4\n",
    "seq_length = 101\n",
    "convolution_window = 10\n",
    "n_filters = 15\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv1D(n_filters, convolution_window,\n",
    "                  activation='relu', padding='same',\n",
    "                  input_shape=(seq_length, features)))\n",
    "\n",
    "model2.add(Conv1D(n_filters, convolution_window,\n",
    "                  activation='relu', padding='same'))\n",
    "\n",
    "model2.add(MaxPooling1D(3))\n",
    "\n",
    "model2.add(Conv1D(2*n_filters, convolution_window,\n",
    "                  activation='relu', padding='same'))\n",
    "\n",
    "model2.add(Conv1D(2*n_filters, convolution_window,\n",
    "                  activation='relu', padding='same'))\n",
    "\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "\n",
    "model2.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "               sample_weight_mode=None,\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy',\n",
    "                        precision,\n",
    "                        recall,\n",
    "                        fvalue])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOWTaGP4x4Hz"
   },
   "source": [
    "Now fit the model to data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1791
    },
    "colab_type": "code",
    "id": "7KBQGXuN0W37",
    "outputId": "7ab337bd-f69d-4f8c-d566-ec8a98fef403"
   },
   "outputs": [],
   "source": [
    "weights_file_B = os.path.join('models','keras_cnn1d_dna_transcription_B.h5')\n",
    "if os.path.exists(weights_file_B):\n",
    "    model.load_weights(weights_file_B)\n",
    "else:\n",
    "    # Train the model, then evaluate/save the ROC curve\n",
    "    num_epochs = 50\n",
    "    hist2 = model2.fit(X_train, y_train,\n",
    "                     sample_weight = w_train,\n",
    "                     batch_size=2048,\n",
    "                     epochs=num_epochs,\n",
    "                     verbose = 1,\n",
    "                     validation_data=(X_valid, y_valid, w_valid))\n",
    "    model2.save_weights(weights_file_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "5Yni4F3Yx6W8",
    "outputId": "35512af1-c57e-4560-d14d-5cf975d71d84"
   },
   "outputs": [],
   "source": [
    "y_model1 = model.predict(X_test).ravel()\n",
    "fpr1, tpr1, _ = roc_curve(y_test, y_model1)\n",
    "\n",
    "y_model2 = model2.predict(X_test).ravel()\n",
    "fpr2, tpr2, _ = roc_curve(y_test, y_model2)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr1, tpr1, label='Ch 6 arch', color='b')\n",
    "plt.plot(fpr2, tpr2, label='Keras ex arch', color='r')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "KfRewRlGTVTy",
    "outputId": "77db7288-7876-4fcd-e32f-31f9b3c17fa5"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['loss'])\n",
    "plt.plot(hist2.history['val_loss'])\n",
    "plt.title('Loss Rate (Keras Example Arch)', size=14)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training interations')\n",
    "plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist2.history['acc'])\n",
    "plt.plot(hist2.history['val_acc'])\n",
    "plt.title('Accuracy Rate (Keras Example Arch)', size=14)\n",
    "plt.ylabel('Accuracy %')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.legend(['Training','Validation'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "q5J9GgMqTg3-",
    "outputId": "6bd33d1f-c618-4ff6-f602-453e27a6d5f2"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['precision'])\n",
    "plt.plot(hist2.history['val_precision'])\n",
    "plt.title('Precision (Keras Example Arch)', size=14)\n",
    "plt.ylabel('Precision %')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.legend(['Training','Validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(hist2.history['recall'])\n",
    "plt.plot(hist2.history['val_recall'])\n",
    "plt.title('Recall (Keras Example Arch)', size=14)\n",
    "plt.ylabel('Recall %')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.legend(['Training','Validation'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "dpdPLhGdTjzJ",
    "outputId": "82432d4e-4b85-4746-e6eb-bf8697b24363"
   },
   "outputs": [],
   "source": [
    "plt.plot(hist2.history['fvalue'])\n",
    "plt.plot(hist2.history['val_fvalue'])\n",
    "plt.title('F-Value (Keras Example Arch)', size=14)\n",
    "plt.ylabel('F-Value')\n",
    "plt.xlabel('Training iterations')\n",
    "plt.legend(['Training','Validation'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3qf8ZSZxe1Mr",
    "outputId": "4685976c-8a8e-40a9-b578-a683e7f0825e"
   },
   "outputs": [],
   "source": [
    "# True values: y_test\n",
    "# Pred values: y_chmodel\n",
    "cm2 = confusion_matrix(y_test, np.round(y_model2))\n",
    "print(cm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "XhEdOZlee47e",
    "outputId": "e7ada9c2-25b2-4a84-d835-5ac2148f2323"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "plot_confusion_matrix(ax, y_test, np.round(y_model2), classes=['N','Y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "WJ8HsSVY_Usb",
    "outputId": "91755fd1-cecd-4490-8e07-c3271aea8c07"
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ds5BMMB9Ay8E",
    "outputId": "fb8170b6-e03d-49f4-e1ee-863d60c8feac"
   },
   "outputs": [],
   "source": [
    "print(np.max(y_model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeM8AdM8zhu3"
   },
   "source": [
    "# Adding Chromatin Accessibility Data\n",
    "\n",
    "The poor performance of the sequential model is no reason to give up. Rather than fiddling with the layers and parameters, we should start by adding more data to the model.\n",
    "\n",
    "Towards this end, we provide the model with additional data about availability of chromatin for each sequence. This requires making modifications to the neural network so it can take multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "GSJZiLrZzCFi",
    "outputId": "6cf22b81-ba96-4c16-8cca-386ede2e4e1c"
   },
   "outputs": [],
   "source": [
    "!ls DeepLearningLifeSciences/Chapter06/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0v139UjjBuWp"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHserwDQCCFI"
   },
   "outputs": [],
   "source": [
    "span_accessibility = {}\n",
    "for line in open('chromatin.txt','r'):\n",
    "    fields = line.split()\n",
    "    span_accessibility[fields[0]] = float(fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDkSew3qaoVV"
   },
   "outputs": [],
   "source": [
    "def get_accessibility_data():\n",
    "    \"\"\"Load the chromatin accessibility data\n",
    "    and return it as a Python dictionary indexed\n",
    "    by sequence IDs.\n",
    "    \"\"\"\n",
    "    # Load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    return accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6jJ13aANTw3r"
   },
   "source": [
    "Now we need to add an additional input layer into the model. The book does this by adding one feature layer with one feature (the chromatin accessibility value), then concatenating the input to the final Flatten layer.\n",
    "\n",
    "We build a new chromatin model with the book's architecture. However, this time we build the model differently: we use the Keras functional API (where layers are called as functions, with the prior layer passed as arguments). This allows us to create a layer that concatenates multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IwEdTuI4CN0d"
   },
   "outputs": [],
   "source": [
    "features = 4\n",
    "seq_length = 101\n",
    "convolution_window = 10\n",
    "n_filters = 15\n",
    "\n",
    "# The following creates a model using the Keras functional API\n",
    "# (instead of using Sequential() and adding layers one at a time)\n",
    "\n",
    "# Sequence portion of the neural net\n",
    "sequence_input = Input(shape=(seq_length,features))\n",
    "\n",
    "\n",
    "# Layers for sequential data\n",
    "# \n",
    "# Layer 1\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "            activation='relu', \n",
    "            padding='same')(sequence_input)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "# Layer 2\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "                activation='relu', \n",
    "                padding='same')(seq)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "# Layer 3\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "                activation='relu', \n",
    "                padding='same')(seq)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "# Flatten to 1D\n",
    "seq = Flatten()(seq)\n",
    "\n",
    "# Assemble the sequential network\n",
    "seq = keras.Model(inputs=sequence_input, outputs=seq)\n",
    "\n",
    "# Layer for chromatin input\n",
    "chromatin_input = Input(shape=(1,))\n",
    "\n",
    "# Create a concatenation layer\n",
    "# that combines the output of the \n",
    "# 1D CNN with the chromatin coverage value\n",
    "fin = keras.layers.concatenate([seq.output,chromatin_input])\n",
    "\n",
    "fin = Dense(1,activation='sigmoid')(fin)\n",
    "\n",
    "chromatin_model = keras.Model(inputs=[seq.input,chromatin_input], outputs=fin)\n",
    "\n",
    "# Now compile the model\n",
    "chromatin_model.compile(loss='binary_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        sample_weight_mode=None,\n",
    "                        metrics=['accuracy',\n",
    "                                 precision,\n",
    "                                 recall,\n",
    "                                 fvalue])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8T-t3kNc1JJ"
   },
   "source": [
    "Now our `chromatin_model` takes two inputs: one is our sequential input (as before), the other is the chromatin accessibility data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0TX59RXzsy46"
   },
   "source": [
    "### Keras Data Generator\n",
    "\n",
    "In the DeepChem book, the custom `DiskDataset` class provided by DeepChem has a built-in method to iterate through the data in batches. Because we've switched to Keras, we have extra work to do to implement our own data generator function.\n",
    "\n",
    "The sequence data generator can take any input arguments; each time it is called it should return a batch of data. When we fit our model to data, we should use the model function `fit_generator()` instead of `fit()`.\n",
    "\n",
    "Hat tip to a [pyimagesearch blog post](https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/) on this topic!\n",
    "\n",
    "Also note, as per [this comment](https://github.com/keras-team/keras/issues/3653#issuecomment-426724197) in an issue in the keras repo, if the model is expecting weights, the generator should return weights as the last item in the tuple returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7-bMv3rf6ak"
   },
   "outputs": [],
   "source": [
    "# A generator function that generates\n",
    "# batches of input (sequence-plus-chromatin)\n",
    "# and output (0/1 labels) data\n",
    "\n",
    "def sequence_data_generator(mode, dataset, batch_size):\n",
    "    \"\"\"Generator function that returns one batch of data per call.\n",
    "    The neural net model takes two inputs, so this should\n",
    "    return a tuple, with the first input (one-hot encoded DNA \n",
    "    sequence) as the first item and the second input (chromatin\n",
    "    accessibility) as the second item.\n",
    "    \n",
    "    Args:\n",
    "        mode               String indicating generator mode\n",
    "                           (\"train\" loops infinitely, \"eval\" stops at end)\n",
    "        dataset            Which data set to load\n",
    "                           (\"train\", \"valid\", \"test\")\n",
    "        batch_size         Size of batch data returned\n",
    "    \"\"\"\n",
    "    if mode not in [\"train\",\"eval\"]:\n",
    "        raise Exception(\"Invalid mode specified: must be 'train' or 'eval'\")\n",
    "    if dataset not in [\"train\",\"valid\",\"test\"]:\n",
    "        raise Exception(\"Invalid dataset specified: must be 'train', 'valid', 'test'\")\n",
    "    \n",
    "    # Because our data set is small enough,\n",
    "    # we start by loading everything into\n",
    "    # memory, and loop over it to return batches.\n",
    "    data_dir = \"%s_dataset\"%(dataset)\n",
    "    base_filename = \"shard-0-%s.joblib\"\n",
    "    \n",
    "    # Load sequence data\n",
    "    X   = joblib.load(os.path.join(data_dir, base_filename%(\"X\")))\n",
    "    y   = joblib.load(os.path.join(data_dir, base_filename%(\"y\")))\n",
    "    w   = joblib.load(os.path.join(data_dir, base_filename%(\"w\")))\n",
    "    ids = joblib.load(os.path.join(data_dir, base_filename%(\"ids\")))\n",
    "    \n",
    "    w = np.squeeze(w)\n",
    "    \n",
    "    # Load chromatin accessibility data\n",
    "    accessibility = get_accessibility_data()\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    chrom_accessibility = np.array([accessibility[k] for k in ids])\n",
    "        \n",
    "    # Get some useful numbers\n",
    "    n_obs     = np.shape(X)[0]\n",
    "    n_batches = math.ceil(n_obs/batch_size)\n",
    "    if batch_size > n_obs:\n",
    "        raise Exception(\"Invalid batch_size specified: larger than data set!\")\n",
    "    \n",
    "\n",
    "    # Sequence data generators for keras \n",
    "    # should be infinite for training mode\n",
    "    # and should stop for evaluation mode\n",
    "    \n",
    "    batch_start = 0\n",
    "    batch_end = batch_start + batch_size\n",
    "    \n",
    "    # To assemble the batch:\n",
    "    # - get the X and y values corresponding to these observations\n",
    "    # - get the sequence IDs corresponding to observations\n",
    "    # - convert the sequence IDs to a chromatin accessibility vector\n",
    "    X_batch = X[batch_start:batch_end]\n",
    "    y_batch = y[batch_start:batch_end]\n",
    "    w_batch = w[batch_start:batch_end]\n",
    "    ids_batch = ids[batch_start:batch_end]\n",
    "    acc_batch = np.array([accessibility[k] for k in ids_batch])\n",
    "    yield ([X_batch,acc_batch], y_batch, w_batch)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Update indices\n",
    "        batch_start = batch_end\n",
    "        batch_end = batch_start + batch_size\n",
    "        \n",
    "        # Left and right halves of batch (X and chromatin access)\n",
    "        X_batch = None\n",
    "        y_batch = None\n",
    "        w_batch = None\n",
    "        ids_batch = None\n",
    "        acc_batch = None\n",
    "        \n",
    "        if batch_end < n_obs:\n",
    "            \n",
    "            # Not at the end of the data set yet\n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            y_batch = y[batch_start:batch_end]\n",
    "            w_batch = w[batch_start:batch_end]\n",
    "            ids_batch = ids[batch_start:batch_end]\n",
    "            acc_batch = np.array([accessibility[k] for k in ids_batch])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Reached end of data set\n",
    "            if mode==\"train\":\n",
    "                # Loop back around forever\n",
    "                batch_end = batch_end%n_obs\n",
    "                sss = X[batch_start:]\n",
    "                eee = X[:batch_end]\n",
    "                X_batch   = np.concatenate([X[batch_start:],X[:batch_end]])\n",
    "                y_batch   = np.concatenate([y[batch_start:],y[:batch_end]])\n",
    "                w_batch   = np.concatenate([w[batch_start:],w[:batch_end]])\n",
    "                ids_batch = np.concatenate([ids[batch_start:], ids[:batch_end]])\n",
    "                acc_batch = np.array([accessibility[k] for k in ids_batch])\n",
    "                \n",
    "            elif mode==\"eval\":\n",
    "                # Reset counter\n",
    "                batch_end = 0\n",
    "                # Stop short at the end\n",
    "                X_batch   = X[batch_start:]\n",
    "                y_batch   = y[batch_start:]\n",
    "                w_batch   = y[batch_start:]\n",
    "                ids_batch = ids[batch_start:]\n",
    "                acc_batch = np.array([accessibility[k] for k in ids_batch])\n",
    "\n",
    "        # Return a list of two inputs (to match the neural net architecture)\n",
    "        # and one output, glued together in a tuple\n",
    "        yield ([X_batch,acc_batch],y_batch,w_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4F6-tQxg41j"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "# We already know the size of the training, validation, and test sets\n",
    "# because we loaded them into memory earlier in the notebook, and did this:\n",
    "# n_train_obs = np.shape(X_train)[0]\n",
    "# n_valid_obs = np.shape(X_valid)[0]\n",
    "# n_test_obs = np.shape(X_test)[0]\n",
    "# \n",
    "# However, to use a generator to get data from on disk, we would need\n",
    "# metadata about the size of the dataset.\n",
    "\n",
    "training_generator = sequence_data_generator(mode=\"train\", \n",
    "                                             dataset=\"train\", \n",
    "                                             batch_size=batch_size)\n",
    "\n",
    "validation_generator = sequence_data_generator(mode=\"train\", \n",
    "                                               dataset=\"valid\", \n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "testing_generator = sequence_data_generator(mode=\"eval\",\n",
    "                                            dataset=\"test\",\n",
    "                                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we specified sample-wise weights, and the data generator we wrote above will load/return X, y, and weights.\n",
    "\n",
    "(Also note that there are two weights here - the `load_weights()` and `save_weights()` functions below refer to the weights of the neural network, not the weights of the input data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1774
    },
    "colab_type": "code",
    "id": "WQlYgO5SaQRb",
    "outputId": "9a76500e-6dd6-4f9c-c74e-3fc005911100"
   },
   "outputs": [],
   "source": [
    "weights_file_C = os.path.join('models','keras_cnn1d_dna_transcription_C.h5')\n",
    "if os.path.exists(weights_file_C):\n",
    "    chromatin_model.load_weights(weights_file_C)\n",
    "else:\n",
    "    # Fit the model to the data using the data generator\n",
    "    num_epochs = 50\n",
    "    ch_hist = chromatin_model.fit_generator(training_generator,\n",
    "                                            steps_per_epoch = n_train_obs // batch_size,\n",
    "                                            validation_data = validation_generator,\n",
    "                                            validation_steps = n_valid_obs // batch_size,\n",
    "                                            epochs = num_epochs)\n",
    "    chromatin_model.save_weights(weights_file_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMGaNfxUc2CP"
   },
   "outputs": [],
   "source": [
    "y_chmodel = chromatin_model.predict_generator(testing_generator, steps=(n_test_obs//batch_size) + 1)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_chmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "TSppOkAAo7vD",
    "outputId": "ae5a0c10-8a25-4ab0-fd03-12019931ce0f"
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='Chrom Acc 1D CNN')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve (with chromatin accessibility)')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "-mBq0gKYaE1B",
    "outputId": "fc390679-8014-4efa-9a8f-7b5509b7dd0b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(ch_hist.history['loss'])\n",
    "    plt.plot(ch_hist.history['val_loss'])\n",
    "    plt.title('Loss Rate (w Chrom Access Data)', size=14)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Training interations')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ch_hist.history['acc'])\n",
    "    plt.plot(ch_hist.history['val_acc'])\n",
    "    plt.title('Accuracy Rate (w Chrom Access Data)', size=14)\n",
    "    plt.ylabel('Accuracy %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "colab_type": "code",
    "id": "MSkqYrH1aR0v",
    "outputId": "cce310a0-cff5-4cd8-b6bb-d4cbf25b2dfd"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(ch_hist.history['precision'])\n",
    "    plt.plot(ch_hist.history['val_precision'])\n",
    "    plt.title('Precision (w Chrom Access Data)', size=14)\n",
    "    plt.ylabel('Precision %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(ch_hist.history['recall'])\n",
    "    plt.plot(ch_hist.history['val_recall'])\n",
    "    plt.title('Recall (w Chrom Access Data)', size=14)\n",
    "    plt.ylabel('Recall %')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "TXldQouMaaMh",
    "outputId": "451a0574-b3d7-4131-9bd6-c837e101bc3f"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(ch_hist.history['fvalue'])\n",
    "    plt.plot(ch_hist.history['val_fvalue'])\n",
    "    plt.title('F-Value (Keras Example Arch)', size=14)\n",
    "    plt.ylabel('F-Value')\n",
    "    plt.xlabel('Training iterations')\n",
    "    plt.legend(['Training','Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"Warning: this model was loaded from a file No training history was saved or loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "2b-rMOMRdnNR",
    "outputId": "ba20f406-8fef-4326-bca4-6de4920073ec"
   },
   "outputs": [],
   "source": [
    "# True values: y_test\n",
    "# Pred values: y_chmodel\n",
    "ch_cm = confusion_matrix(y_test, np.round(y_chmodel))\n",
    "print(ch_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "colab_type": "code",
    "id": "OZMvcLG0ewGx",
    "outputId": "d82a645f-c80b-4c45-dd15-404f95c3603e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,7))\n",
    "plot_confusion_matrix(ax, y_test, np.round(y_chmodel), classes=['N','Y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "9lyOzTGuBmWA",
    "outputId": "9b561ece-1ae8-48a3-a6ae-e6d9f6834a7f"
   },
   "outputs": [],
   "source": [
    "sns.distplot(y_chmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lLuPGYEzBofY",
    "outputId": "8fbbc52a-aaf0-4de7-b2d4-2b0d15a965a9"
   },
   "outputs": [],
   "source": [
    "print(np.max(y_chmodel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`----------------------------------------------`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LWfHtYJjCfmg"
   },
   "outputs": [],
   "source": [
    "features = 4\n",
    "seq_length = 101\n",
    "convolution_window = 32\n",
    "n_filters = 64\n",
    "\n",
    "# The following creates a model using the Keras functional API\n",
    "# (instead of using Sequential() and adding layers one at a time)\n",
    "\n",
    "# Sequence portion of the neural net\n",
    "sequence_input = Input(shape=(seq_length,features))\n",
    "\n",
    "\n",
    "# Layers for sequential data\n",
    "# \n",
    "# Layer 1\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "           activation='relu', kernel_initializer='normal', padding='same')(sequence_input)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "           activation='relu', kernel_initializer='normal', padding='same')(seq)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "seq = Conv1D(n_filters, convolution_window, \n",
    "           activation='relu', kernel_initializer='normal', padding='same')(seq)\n",
    "seq = Dropout(0.5)(seq)\n",
    "\n",
    "seq = Flatten()(seq)\n",
    "seq = keras.Model(inputs=sequence_input, outputs=seq)\n",
    "chromatin_input = Input(shape=(1,))\n",
    "fin = keras.layers.concatenate([seq.output,chromatin_input])\n",
    "\n",
    "# (add a kernel initializer)\n",
    "fin = Dense(1, kernel_initializer='normal', activation='sigmoid')(fin)\n",
    "\n",
    "chromatin_model2 = keras.Model(inputs=[seq.input,chromatin_input], outputs=fin)\n",
    "\n",
    "# Now compile the model\n",
    "chromatin_model2.compile(loss='binary_crossentropy',\n",
    "                        optimizer='adam',\n",
    "                        sample_weight_mode=None,\n",
    "                        metrics=['accuracy',\n",
    "                                 precision,\n",
    "                                 recall,\n",
    "                                 fvalue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ze1N_PXuC0O1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "keras_cnn1d_dna_transcription.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
